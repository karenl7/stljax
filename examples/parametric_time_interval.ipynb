{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "\n",
    "from stljax.formula import Predicate, DifferentiableAlways\n",
    "from stljax.utils import smooth_mask\n",
    "\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Palatino\"]})\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "\n",
    "Generate noisy trajectories resembling a \"bump\" between (normalized) time [t_start, t_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "fontsize = 14\n",
    "true_t_start = 0.21\n",
    "true_t_end = 0.59\n",
    "bs = 32\n",
    "\n",
    "\n",
    "key = jax.random.key(1701)\n",
    "signal_data = (\n",
    "    jax.vmap(smooth_mask, [None, 0, None, None])(\n",
    "        T, true_t_start + 0.02 * jax.random.normal(key, shape=(bs,)), true_t_end, 3.0\n",
    "    )\n",
    "    + jax.random.normal(\n",
    "        key,\n",
    "        shape=(\n",
    "            bs,\n",
    "            T,\n",
    "        ),\n",
    "    )\n",
    "    * 0.1\n",
    "    - 0.5\n",
    ")\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(jnp.linspace(0, 1, T), signal_data.T, color=\"black\", alpha=0.2)\n",
    "plt.xlabel(\"Normalized time\", fontsize=fontsize, labelpad=-1)\n",
    "plt.ylabel(\"Signal\", fontsize=fontsize, labelpad=-1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the STL formula with differentiable time intervals and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Predicate(\"x\", lambda x: x)  # define a predicate that is the identity function\n",
    "phi = DifferentiableAlways(pred > 0.0) # define phi using differentiable time interval\n",
    "\n",
    "# define robustness of phi, and jit it for performance\n",
    "phi_robustness_jit = jax.jit(phi.robustness, static_argnames=(\"approx_method\"))\n",
    "\n",
    "# maximize robustness and maximize time interval\n",
    "@functools.partial(jax.jit, static_argnames=(\"approx_method\"))\n",
    "def loss(signal_data, t_start, t_end, scale, approx_method, temperature, coeff):\n",
    "    rob_partial = functools.partial(\n",
    "        phi_robustness_jit,\n",
    "        t_start=t_start,\n",
    "        t_end=t_end,\n",
    "        scale=scale,\n",
    "        approx_method=approx_method,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    robustness_ = jax.vmap(rob_partial, [0])(signal_data)\n",
    "    # nan entries if t_start >= t_end - 0.05\n",
    "    robustness = jax.nn.relu(\n",
    "        -jnp.where(t_start < (t_end - 0.05), robustness_, jnp.nan)\n",
    "    ).mean()\n",
    "    return robustness + coeff * (t_start - t_end)\n",
    "\n",
    "# define gradient of the loss function with respect to t_start and t_end\n",
    "# this is used to update the parameters t_start and t_end during optimization\n",
    "grad_loss = jax.jit(jax.grad(loss, [1, 2]), static_argnames=(\"approx_method\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up gradient descent routine and perform gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_method = \"logsumexp\"\n",
    "a = -2.0 # starting point for t_start, to be passed through sigmoid\n",
    "b = 2.0 # starting point for t_end, to be passed through sigmoid\n",
    "lr = 1e-2\n",
    "max_steps = 5000\n",
    "scale_start = 0.1 # starting scale value for the smooth mask approximation\n",
    "scale_end = 20 # ending scale value for the smooth mask approximation\n",
    "temperature_start = 0.1 # starting temperature for the logsumexp approximation\n",
    "temperature_end = 20 # ending temperature for the logsumexp approximation\n",
    "a_list = [a] # list to store t_start values during optimization\n",
    "b_list = [b] # list to store t_end values during optimization\n",
    "coeff_start = 0.1 # starting coefficient for the time interval penalty\n",
    "coeff_end = 0.0 # ending coefficient for the time interval penalty\n",
    "\n",
    "# Gradient descent!\n",
    "for i in range(max_steps):\n",
    "    j = i / max_steps\n",
    "    s = (1 - j) * scale_start + j * scale_end # linear schedule for scale\n",
    "    t = (1 - j) * temperature_start + j * temperature_end # linear schedule for temperature\n",
    "    c = (1 - j) * coeff_start + j * coeff_end # linear schedule for coefficient\n",
    "    a_ = jax.nn.sigmoid(a) # apply sigmoid to t_start\n",
    "    b_ = jax.nn.sigmoid(b) # apply sigmoid to t_end\n",
    "    ga, gb = grad_loss(signal_data, a_, b_, s, approx_method, t, c)\n",
    "    a -= lr * ga * a_ * (1 - a_)\n",
    "    b -= lr * gb * b_ * (1 - b_)\n",
    "    a_list.append(a)\n",
    "    b_list.append(b)\n",
    "    # print(a,b)\n",
    "a_list = jnp.stack(a_list)\n",
    "b_list = jnp.stack(b_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 50\n",
    "coeff = 0.1\n",
    "\n",
    "def visualize_loss_landscape(ax,\n",
    "    loss_func, signal, scale, approx_method, temperature, coeff\n",
    "):\n",
    "    N = 100\n",
    "    fontsize = 14\n",
    "    levels = 10\n",
    "    starts, ends = jnp.meshgrid(jnp.linspace(0, 1, N), jnp.linspace(0, 1, N))\n",
    "    losses = jax.vmap(loss_func, [None, 0, 0, None, None, None, None])(\n",
    "        signal,\n",
    "        starts.reshape([-1, 1]),\n",
    "        ends.reshape([-1, 1]),\n",
    "        scale,\n",
    "        approx_method,\n",
    "        temperature,\n",
    "        coeff,\n",
    "    ).reshape([N, N])\n",
    "\n",
    "    ax.contourf(starts, ends, losses, levels=levels, cmap=\"jet\", alpha=0.4)\n",
    "    # plt.colorbar()\n",
    "    if approx_method != \"true\":\n",
    "        match approx_method:\n",
    "            case \"logsumexp\":\n",
    "                app = \"LSE\"\n",
    "            case \"softmax\":\n",
    "                app = \"soft\"\n",
    "        ax.set_title(\n",
    "            \"Loss landscape \\n  (c = %.2f, $\\\\tau_\\\\mathrm{%s}$ = %.2f)\"\n",
    "            % (scale, app, temperature)\n",
    "        )\n",
    "    else:\n",
    "        ax.set_title(\"Loss landscape \\n (c = %.2f)\" % (scale))\n",
    "    ax.set_xlabel(\"$a$\", fontsize=fontsize, labelpad=-3)\n",
    "    ax.set_ylabel(\"$b$\", fontsize=fontsize, labelpad=-3)\n",
    "    ax.grid(zorder=-5, alpha=0.2)\n",
    "\n",
    "\n",
    "def visualize_results(ax, i, approx_method):\n",
    "    a, b = jax.nn.sigmoid(a_list[i]), jax.nn.sigmoid(b_list[i])\n",
    "    j = i / max_steps\n",
    "    s = (1 - j) * scale_start + j * scale_end\n",
    "    t = (1 - j) * temperature_start + j * temperature_end\n",
    "    ell = loss(signal_data, a, b, s, approx_method, t, coeff)\n",
    "\n",
    "    # plt.figure(figsize=(4,3))\n",
    "\n",
    "    visualize_loss_landscape(ax, loss, signal_data, s, approx_method, t, coeff)\n",
    "    ax.plot(\n",
    "        jax.nn.sigmoid(a_list[::step_size]),\n",
    "        jax.nn.sigmoid(b_list[::step_size]),\n",
    "        \"o-\",\n",
    "        markersize=3,\n",
    "        color=\"black\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    current_loss = loss(signal_data, a, b, s, approx_method, t, coeff)\n",
    "    gt_loss = loss(signal_data, true_t_start, true_t_end, s, approx_method, t, coeff)\n",
    "    ax.scatter(\n",
    "        [a],\n",
    "        [b],\n",
    "        marker=\"o\",\n",
    "        s=40,\n",
    "        color=\"magenta\",\n",
    "        label=\"Current: %.3f\" % current_loss,\n",
    "        edgecolor=\"black\",\n",
    "        zorder=4,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        [true_t_start],\n",
    "        [true_t_end],\n",
    "        marker=\"*\",\n",
    "        s=100,\n",
    "        color=\"orange\",\n",
    "        label=\"Ground truth: %.3f\" % gt_loss,\n",
    "        edgecolor=\"black\",\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    ax.vlines(a, 0, 1, color=\"red\", linestyle=\"--\")\n",
    "    ax.hlines(b, 0, 1, color=\"blue\", linestyle=\"--\")\n",
    "    if approx_method != \"true\":\n",
    "        match approx_method:\n",
    "            case \"logsumexp\":\n",
    "                app = \"LSE\"\n",
    "            case \"softmax\":\n",
    "                app = \"soft\"\n",
    "        ax.set_title(\n",
    "            f\"Step {i}, \" + \"Loss = %.5f (c = %.2f, $\\\\tau_\\\\mathrm{%s}$ = %.2f)\" % (ell, s, app, t)\n",
    "        )\n",
    "    else:\n",
    "        ax.set_title(f\"Step {i}, \" + \"Loss = %.5f (c = %.2f)\" % (ell, s))\n",
    "    ax.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "step_size = 100\n",
    "\n",
    "def animate(idx):\n",
    "    ax.clear()\n",
    "    visualize_results(ax, idx, approx_method)\n",
    "    # ax.set_title(f\"Step {idx}\")\n",
    "\n",
    "frames = range(0, max_steps, step_size)\n",
    "ani = animation.FuncAnimation(fig, animate, frames=frames, interval=100, repeat=False)\n",
    "\n",
    "plt.close(fig)  # Prevent duplicate static plot in notebook output\n",
    "HTML(ani.to_jshtml()) # Display the animation in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

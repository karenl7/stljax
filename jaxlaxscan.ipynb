{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from stljax.formula import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Operator2(STL_Formula):\n",
    "    \"\"\"\n",
    "    Class to compute Eventually and Always. This builds a recurrent cell to perform dynamic programming\n",
    "\n",
    "    Args:\n",
    "        subformula: The subformula that the temporal operator is applied to.\n",
    "        interval: The time interval that the temporal operator operates on. Default: None which means [0, jnp.inf]. Other options car be: [a, b] (b < jnp.inf), [a, jnp.inf] (a > 0)\n",
    "\n",
    "    NOTE: Assume that the interval is describing the INDICES of the desired time interval. The user is responsible for converting the time interval (in time units) into indices (integers) using knowledge of the time step size.\n",
    "    \"\"\"\n",
    "    def __init__(self, subformula, interval=None):\n",
    "        super().__init__()\n",
    "        self.subformula = subformula\n",
    "        self.interval = interval\n",
    "        self._interval = [0, jnp.inf] if self.interval is None else self.interval\n",
    "        self.hidden_dim = 1 if not self.interval else self.interval[-1]    # hidden_dim=1 if interval is [0, ∞) otherwise hidden_dim=end of interval\n",
    "        if self.hidden_dim == jnp.inf:\n",
    "            self.hidden_dim = self.interval[0]\n",
    "        self.steps = 1 if not self.interval else self.interval[-1] - self.interval[0] + 1   # steps=1 if interval is [0, ∞) otherwise steps=length of interval\n",
    "        self.operation = None\n",
    "        # Matrices that shift a vector and add a new entry at the end.\n",
    "        self.M = jnp.diag(jnp.ones(self.hidden_dim-1), k=1)\n",
    "        self.b = jnp.zeros(self.hidden_dim)\n",
    "        self.b = self.b.at[-1].set(1)\n",
    "        self.LARGE_NUMBER = 1E9\n",
    "\n",
    "    def _cell(self, x, hidden_state, **kwargs):\n",
    "        \"\"\"\n",
    "        This function describes the operation that takes place at each recurrent step.\n",
    "        Args:\n",
    "            x: the input state at time t [batch_size, 1, ...]\n",
    "            hidden_state: the hidden state. It is either a tensor, or a tuple of tensors, depending on the interval chosen and other arguments. Generally, the hidden state is of size [batch_size, hidden_dim,...]\n",
    "\n",
    "        Return:\n",
    "            output and next hidden_state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"_cell is not implemented\")\n",
    "\n",
    "    def _run_cell(self, signal, padding, **kwargs):\n",
    "        \"\"\"\n",
    "        Function to run a signal through a cell T times, where T is the length of the signal in the time dimension\n",
    "\n",
    "        Args:\n",
    "            signal: input signal, size = [time_dim,]\n",
    "            time_dim: axis corresponding to time_dim. Default: 0\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Return:\n",
    "            outputs: list of outputs\n",
    "            states: list of hidden_states\n",
    "        \"\"\"\n",
    "        time_dim = 0  # assuming signal is [time_dim,...]\n",
    "        outputs = []\n",
    "        states = []\n",
    "        hidden_state = self._initialize_hidden_state(signal, padding=padding) # [hidden_dim]\n",
    "        signal_split = jnp.split(signal, signal.shape[time_dim], time_dim)    # list of x at each time step\n",
    "        \n",
    "        for i in range(signal.shape[time_dim]):\n",
    "            o, hidden_state = self._cell(signal_split[i], hidden_state, **kwargs)\n",
    "            outputs.append(o)\n",
    "            states.append(hidden_state)\n",
    "        return outputs, states\n",
    "\n",
    "\n",
    "\n",
    "    def _robustness_trace(self, signal, padding, **kwargs):\n",
    "        \"\"\"\n",
    "        Function to compute robustness trace of a temporal STL formula\n",
    "        First, compute the robustness trace of the subformula, and use that as the input for the recurrent computation\n",
    "\n",
    "        Args:\n",
    "            signal: input signal, size = [bs, time_dim, ...]\n",
    "            time_dim: axis corresponding to time_dim. Default: 1\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Returns:\n",
    "            robustness_trace: jnp.array. Same size as signal.\n",
    "        \"\"\"\n",
    "        time_dim = 0  # assuming signal is [time_dim,...]\n",
    "        trace = self.subformula(signal, **kwargs)\n",
    "        outputs, _ = self._run_cell(trace, padding, **kwargs)\n",
    "        return jnp.concatenate(outputs, axis=time_dim)     \n",
    "\n",
    "    def robustness(self, signal, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the robustness value. Extracts the last entry along time_dim of robustness trace.\n",
    "\n",
    "        Args:\n",
    "            signal: jnp.array or Expression. Expected size [bs, time_dim, state_dim]\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Return: jnp.array, same as input with the time_dim removed.\n",
    "        \"\"\"\n",
    "        return self.__call__(signal, **kwargs)[-1]\n",
    "        # return jnp.rollaxis(self.__call__(signal, **kwargs), time_dim)[-1]\n",
    "\n",
    "    def _next_function(self):\n",
    "        \"\"\" next function is the input subformula. For visualization purposes \"\"\"\n",
    "        return [self.subformula]\n",
    "\n",
    "class AlwaysRecurrent2(Temporal_Operator):\n",
    "    \"\"\"\n",
    "    The Always STL formula □_[a,b] subformula\n",
    "    The robustness value is the minimum value of the input trace over a prespecified time interval\n",
    "\n",
    "    Args:\n",
    "        subformula: subformula that the Always operation is applied on\n",
    "        interval: time interval [a,b] where a, b are indices along the time dimension. It is up to the user to keep track of what the timestep size is.\n",
    "    \"\"\"\n",
    "    def __init__(self, subformula, interval=None):\n",
    "        super().__init__(subformula=subformula, interval=interval)\n",
    "\n",
    "\n",
    "    def _initialize_hidden_state(self, signal, padding):\n",
    "        \"\"\"\n",
    "        Compute the initial hidden state.\n",
    "\n",
    "        Args:\n",
    "            signal: the input signal. Expected size [time_dim,]\n",
    "\n",
    "        Returns:\n",
    "            h0: initial hidden state is [hidden_dim,]\n",
    "\n",
    "        Notes:\n",
    "        Initializing the hidden state requires padding on the signal. Currently, the default is to extend the last value.\n",
    "        TODO: have option on this padding\n",
    "\n",
    "        \"\"\"\n",
    "        # Case 1, 2, 4\n",
    "        # TODO: make this less hard-coded. Assumes signal is [bs, time_dim, signal_dim], and already reversed\n",
    "        # pads with the signal value at the last time step.\n",
    "        if padding == \"last\":\n",
    "            pad_value = jax.lax.stop_gradient(signal)[0]\n",
    "        elif padding == \"mean\":\n",
    "            pad_value = jax.lax.stop_gradient(signal).mean(0)\n",
    "        else:\n",
    "            pad_value = self.LARGE_NUMBER\n",
    "\n",
    "        h0 = jnp.ones([self.hidden_dim, *signal.shape[1:]]) * pad_value\n",
    "\n",
    "        # Case 3: if self.interval is [a, jnp.inf), then the hidden state is a tuple (like in an LSTM)\n",
    "        if (self._interval[1] == jnp.inf) & (self._interval[0] > 0):\n",
    "            c0 = signal[:1]\n",
    "            return (c0, h0)\n",
    "        return h0\n",
    "\n",
    "    def _cell(self, x, hidden_state, **kwargs):\n",
    "        \"\"\"\n",
    "        see Temporal_Operator._cell\n",
    "        \"\"\"\n",
    "        time_dim = 0  # assuming signal is [time_dim,...]\n",
    "        # Case 1, interval = [0, inf]\n",
    "        if self.interval is None:\n",
    "            input_ = jnp.concatenate([hidden_state, x], axis=time_dim)                # [rnn_dim+1,]\n",
    "            output = minish(input_, time_dim, keepdims=True, **kwargs)       # [1,]\n",
    "            return output, output\n",
    "\n",
    "        # Case 3: self.interval is [a, np.inf)\n",
    "        if (self._interval[1] == jnp.inf) & (self._interval[0] > 0):\n",
    "            c, h = hidden_state\n",
    "            ch = jnp.concatenate([c, h[:1]], axis=time_dim)                             # [2,]\n",
    "            output = minish(ch, time_dim, keepdims=True, **kwargs)               # [1,]\n",
    "            hidden_state_ = (output, self.M @ h + self.b * x)\n",
    "\n",
    "        # Case 2 and 4: self.interval is [a, b]\n",
    "        else:\n",
    "            hidden_state_ = self.M @ hidden_state + self.b * x\n",
    "            hx = jnp.concatenate([hidden_state, x], axis=time_dim)                             # [rnn_dim+1,]\n",
    "            input_ = hx[:self.steps]                               # [self.steps,]\n",
    "            output = minish(input_, time_dim, **kwargs)               # [1,]\n",
    "        return output, hidden_state_\n",
    "\n",
    "    def robustness_trace(self, signal, padding=1E6, **kwargs):\n",
    "        \"\"\"\n",
    "        Function to compute robustness trace of a temporal STL formula\n",
    "        First, compute the robustness trace of the subformula, and use that as the input for the recurrent computation\n",
    "\n",
    "        Args:\n",
    "            signal: input signal, size = [bs, time_dim, ...]\n",
    "            time_dim: axis corresponding to time_dim. Default: 1\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Returns:\n",
    "            robustness_trace: jnp.array. Same size as signal.\n",
    "        \"\"\"\n",
    "        return self._robustness_trace(signal, padding, **kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"◻ \" + str(self._interval) + \"( \" + str(self.subformula) + \" )\"\n",
    "\n",
    "\n",
    "class EventuallyRecurrent2(Temporal_Operator):\n",
    "    \"\"\"\n",
    "    The Eventually STL formula ♢_[a,b] subformula\n",
    "    The robustness value is the minimum value of the input trace over a prespecified time interval\n",
    "\n",
    "    Args:\n",
    "        subformula: subformula that the Eventually operation is applied on\n",
    "        interval: time interval [a,b] where a, b are indices along the time dimension. It is up to the user to keep track of what the timestep size is.\n",
    "    \"\"\"\n",
    "    def __init__(self, subformula, interval=None):\n",
    "        super().__init__(subformula=subformula, interval=interval)\n",
    "\n",
    "    def _initialize_hidden_state(self, signal, padding):\n",
    "        \"\"\"\n",
    "        Compute the initial hidden state.\n",
    "\n",
    "        Args:\n",
    "            signal: the input signal. Expected size [time_dim,]\n",
    "\n",
    "        Returns:\n",
    "            h0: initial hidden state is [hidden_dim,]\n",
    "\n",
    "        Notes:\n",
    "        Initializing the hidden state requires padding on the signal. Currently, the default is to extend the last value.\n",
    "        TODO: have option on this padding\n",
    "\n",
    "        \"\"\"\n",
    "        # Case 1, 2, 4\n",
    "        # TODO: make this less hard-coded. Assumes signal is [bs, time_dim, signal_dim], and already reversed\n",
    "        # pads with the signal value at the last time step.\n",
    "        if padding == \"last\":\n",
    "            pad_value = jax.lax.stop_gradient(signal)[0]\n",
    "        elif padding == \"mean\":\n",
    "            pad_value = jax.lax.stop_gradient(signal).mean(0)\n",
    "        else:\n",
    "            pad_value = -self.LARGE_NUMBER\n",
    "\n",
    "        h0 = jnp.ones([self.hidden_dim, *signal.shape[1:]]) * pad_value\n",
    "\n",
    "        # Case 3: if self.interval is [a, jnp.inf), then the hidden state is a tuple (like in an LSTM)\n",
    "        if (self._interval[1] == jnp.inf) & (self._interval[0] > 0):\n",
    "            c0 = signal[:1]\n",
    "            return (c0, h0)\n",
    "        return h0\n",
    "\n",
    "    def _cell(self, x, hidden_state, **kwargs):\n",
    "        \"\"\"\n",
    "        see Temporal_Operator._cell\n",
    "        \"\"\"\n",
    "        time_dim = 0  # assuming signal is [time_dim,...]\n",
    "        # Case 1, interval = [0, inf]\n",
    "        if self.interval is None:\n",
    "            input_ = jnp.concatenate([hidden_state, x], axis=time_dim)                # [rnn_dim+1, ]\n",
    "            output = maxish(input_, time_dim, keepdims=True, **kwargs)       # [1, ]\n",
    "            return output, output\n",
    "\n",
    "        # Case 3: self.interval is [a, np.inf)\n",
    "        if (self._interval[1] == jnp.inf) & (self._interval[0] > 0):\n",
    "            c, h = hidden_state\n",
    "            ch = jnp.concatenate([c, h[:1]], axis=time_dim)                             # [2, ]\n",
    "            output = maxish(ch, time_dim, keepdims=True, **kwargs)               # [1, ]\n",
    "            hidden_state_ = (output, self.M @ h + self.b * x)\n",
    "\n",
    "        # Case 2 and 4: self.interval is [a, b]\n",
    "        else:\n",
    "            hidden_state_ = self.M @ hidden_state + self.b * x\n",
    "            hx = jnp.concatenate([hidden_state, x], axis=time_dim)                             # [rnn_dim+1, ]\n",
    "            input_ = hx[:self.steps]                               # [self.steps, ]\n",
    "            output = maxish(input_, time_dim, **kwargs)               # [1, ]\n",
    "        return output, hidden_state_\n",
    "\n",
    "    def robustness_trace(self, signal, padding=-1E6, **kwargs):\n",
    "        \"\"\"\n",
    "        Function to compute robustness trace of a temporal STL formula\n",
    "        First, compute the robustness trace of the subformula, and use that as the input for the recurrent computation\n",
    "\n",
    "        Args:\n",
    "            signal: input signal, size = [bs, time_dim, ...]\n",
    "            time_dim: axis corresponding to time_dim. Default: 1\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Returns:\n",
    "            robustness_trace: jnp.array. Same size as signal.\n",
    "        \"\"\"\n",
    "        return self._robustness_trace(signal, padding, **kwargs)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"♢ \" + str(self._interval) + \"( \" + str(self.subformula) + \" )\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# analyzing gradients\n",
    "interval = None\n",
    "pred = Predicate('x', lambda x: x)\n",
    "rec = EventuallyRecurrent((pred > 0.), interval=interval)\n",
    "rec2 = EventuallyRecurrent2((pred > 0.), interval=interval)\n",
    "\n",
    "ev = Eventually((pred > 0.), interval=interval)\n",
    "\n",
    "rec_jit = jax.jit(rec.robustness)\n",
    "rec2_jit = jax.jit(rec2.robustness)\n",
    "ev_jit = jax.jit(ev.robustness)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UntilRecurrent2(STL_Formula):\n",
    "    \"\"\"\n",
    "    The Until STL operator U. Subformula1 U_[a,b] subformula2\n",
    "    Arg:\n",
    "        subformula1: subformula for lhs of the Until operation\n",
    "        subformula2: subformula for rhs of the Until operation\n",
    "        interval: time interval [a,b] where a, b are indices along the time dimension. It is up to the user to keep track of what the timestep is.\n",
    "        overlap: If overlap=True, then the last time step that ϕ is true, ψ starts being true. That is, sₜ ⊧ ϕ and sₜ ⊧ ψ at a common time t. If overlap=False, when ϕ stops being true, ψ starts being true. That is sₜ ⊧ ϕ and sₜ+₁ ⊧ ψ, but sₜ ¬⊧ ψ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subformula1, subformula2, interval=None, overlap=True):\n",
    "        super().__init__()\n",
    "        self.subformula1 = subformula1\n",
    "        self.subformula2 = subformula2\n",
    "        self.interval = interval\n",
    "        if overlap == False:\n",
    "            self.subformula2 = Eventually(subformula=subformula2, interval=[0,1])\n",
    "        self.LARGE_NUMBER = 1E9\n",
    "\n",
    "    def robustness_trace(self, signal, **kwargs):\n",
    "        \"\"\"\n",
    "        Computing robustness trace of subformula1 U subformula2  (see paper)\n",
    "\n",
    "        Args:\n",
    "            signal: input signal for the formula. If using Expressions to define the formula, then inputs a tuple of signals corresponding to each subformula. If using Predicates to define the formula, then inputs is just a single jnp.array. Not need for different signals for each subformula. Expected signal is size [batch_size, time_dim, x_dim]\n",
    "            time_dim: axis for time_dim. Default: 1\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Returns:\n",
    "            robustness_trace: jnp.array. Same size as signal.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # TODO (karenl7) this really assumes axis=1 is the time dimension. Can this be generalized?\n",
    "        time_dim = 0  # assuming signal is [time_dim,...]\n",
    "        LARGE_NUMBER = self.LARGE_NUMBER\n",
    "\n",
    "        if isinstance(signal, tuple):\n",
    "            # for formula defined using Expression\n",
    "            assert signal[0].shape[time_dim] == signal[1].shape[time_dim]\n",
    "            trace1 = self.subformula1(signal[0], **kwargs)\n",
    "            trace2 = self.subformula2(signal[1], **kwargs)\n",
    "            n_time_steps = signal[0].shape[time_dim] # TODO: WIP\n",
    "        else:\n",
    "            # for formula defined using Predicate\n",
    "            trace1 = self.subformula1(signal, **kwargs)\n",
    "            trace2 = self.subformula2(signal, **kwargs)\n",
    "            n_time_steps = signal.shape[time_dim] # TODO: WIP\n",
    "\n",
    "        Alw = AlwaysRecurrent(subformula=Identity(name=str(self.subformula1)))\n",
    "        LHS = jnp.permute_dims(jnp.repeat(jnp.expand_dims(trace2, -1), n_time_steps, axis=-1), [1,0])  # [sub_signal, t_prime]\n",
    "        RHS = jnp.ones_like(LHS) * -LARGE_NUMBER  # [sub_signal, t_prime]\n",
    "\n",
    "        # Case 1, interval = [0, inf]\n",
    "        if self.interval == None:\n",
    "            for i in range(n_time_steps):\n",
    "                RHS = RHS.at[i:,i].set(Alw(trace1[i:], **kwargs))\n",
    "\n",
    "        # Case 2 and 4: self.interval is [a, b], a ≥ 0, b < ∞\n",
    "        elif self.interval[1] < jnp.inf:\n",
    "            a = self.interval[0]\n",
    "            b = self.interval[1]\n",
    "            for i in range(n_time_steps):\n",
    "                end = i+b+1\n",
    "                RHS = RHS.at[i+a:end,i].set(Alw(trace1[i:end], **kwargs)[a:])\n",
    "\n",
    "        # Case 3: self.interval is [a, np.inf), a ≂̸ 0\n",
    "        else:\n",
    "            a = self.interval[0]\n",
    "            for i in range(n_time_steps):\n",
    "                RHS = RHS.at[i+a:,i].set(Alw(trace1[i:], **kwargs)[a:])\n",
    "\n",
    "        return maxish(minish(jnp.stack([LHS, RHS], axis=-1), axis=-1, keepdims=False, **kwargs), axis=-1, keepdims=False, **kwargs)\n",
    "\n",
    "    def robustness(self, signal, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the robustness value. Extracts the last entry along time_dim of robustness trace.\n",
    "\n",
    "        Args:\n",
    "            signal: jnp.array or Expression. Expected size [bs, time_dim, state_dim]\n",
    "            kwargs: Other arguments including time_dim, approx_method, temperature\n",
    "\n",
    "        Return: jnp.array, same as input with the time_dim removed.\n",
    "        \"\"\"\n",
    "        return self.__call__(signal, **kwargs)[-1]\n",
    "        # return jnp.rollaxis(self.__call__(signal, **kwargs), time_dim)[-1]\n",
    "\n",
    "    def _next_function(self):\n",
    "        \"\"\" next function is the input subformulas. For visualization purposes \"\"\"\n",
    "        return [self.subformula1, self.subformula2]\n",
    "\n",
    "    def __str__(self):\n",
    "        return  \"(\" + str(self.subformula1) + \")\" + \" U \" + \"(\" + str(self.subformula2) + \")\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing gradients\n",
    "interval = None\n",
    "pred = Predicate('x', lambda x: x)\n",
    "rec = UntilRecurrent(pred > 0., pred > 0., interval=interval)\n",
    "rec2 = UntilRecurrent2(pred > 0., pred > 0., interval=interval)\n",
    "\n",
    "ev = Until(pred > 0., pred > 0., interval=interval)\n",
    "\n",
    "rec_jit = jax.jit(rec.robustness)\n",
    "rec2_jit = jax.jit(rec2.robustness)\n",
    "ev_jit = jax.jit(ev.robustness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 128\n",
    "bs = 32\n",
    "signal = jnp.array(np.random.randn(bs, T)) * 1.\n",
    "signal_flip = jnp.flip(signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 μs ± 148 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "875 μs ± 98.6 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "2.66 ms ± 72.8 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.vmap(jax.grad(rec_jit))(signal)\n",
    "%timeit jax.vmap(jax.grad(rec2_jit))(signal)\n",
    "%timeit jax.vmap(jax.grad(ev_jit))(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 μs ± 634 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "132 μs ± 2.52 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "1.22 ms ± 7.96 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jax.vmap(rec_jit)(signal)\n",
    "%timeit jax.vmap(rec2_jit)(signal)\n",
    "%timeit jax.vmap(ev_jit)(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
